import org.apache.spark.sql.expressions.MutableAggregationBuffer;
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.*;
import java.math.BigDecimal;
import java.util.Arrays;

public class BigDecimalSum extends UserDefinedAggregateFunction {

    private StructType inputSchema;
    private StructType bufferSchema;

    public BigDecimalSum() {
        this.inputSchema = new StructType().add("value", DataTypes.StringType);
        this.bufferSchema = new StructType().add("result", DataTypes.StringType);
    }

    @Override
    public StructType inputSchema() {
        return inputSchema;
    }

    @Override
    public StructType bufferSchema() {
        return bufferSchema;
    }

    @Override
    public DataType dataType() {
        return DataTypes.StringType;
    }

    @Override
    public boolean deterministic() {
        return true;
    }

    @Override
    public void initialize(MutableAggregationBuffer buffer) {
        buffer.update(0, "0");
    }

    @Override
    public void update(MutableAggregationBuffer buffer, Row input) {
        BigDecimal result = new BigDecimal(buffer.getString(0));
        BigDecimal addend = new BigDecimal(input.getString(0));
        buffer.update(0, result.add(addend).toString());
    }

    @Override
    public void merge(MutableAggregationBuffer buffer1, Row buffer2) {
        BigDecimal result = new BigDecimal(buffer1.getString(0));
        BigDecimal addend = new BigDecimal(buffer2.getString(0));
        buffer1.update(0, result.add(addend).toString());
    }

    @Override
    public Object evaluate(Row buffer) {
        return buffer.getString(0);
    }
}

// 在SparkSession中注册UDAF
spark.udf().register("bigDecimalSum", new BigDecimalSum());

// 将A列转换为string类型，然后进行聚合操作
dataset = dataset.withColumn(A, col(A).cast(DataTypes.StringType));
dataset = dataset.groupBy(col(C)).agg(expr("bigDecimalSum(A)").as(B));

// 使用UDF格式化B列
dataset = dataset.withColumn(B, expr("formatDecimal(B)"));
